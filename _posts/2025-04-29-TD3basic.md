---
title: TD3 (Twin Delay DDPG)简介
description: 从今天开始进军新的强化学习方法，不过这个方法是DDPG的变种，学起来应该不会很难。这个博客和4月25日记录的那个日志中提到的是同一个，是OpenAI写的一个类似教程一样的东西。
author: Chen Zhang
date: 2025-04-29 00:00:00 +0800
categories: [Blogging]
tags: [Reinforcement Learning, TD3, DDPG]
pin: true
math: true
mermaid: true
---

[OpenAI Spinning Up: TD3 (Twin Delayed DDPG)](https://spinningup.openai.com/en/latest/algorithms/td3.html)

**学习与要点摘录：**
- TD3 trains a deterministic policy in an off-policy way. Because the policy is deterministic, if the agent were to explore on-policy, in the beginning it would probably not try a wide enough variety of actions to find useful learning signals. 这句话不太理解。为什么说TD3采用off policy的方式训练，但是在探索的时候又采用on-policy方式。
- To make TD3 policies explore better, we add noise to their actions at training time, typically uncorrelated mean-zero Gaussian noise. To facilitate getting higher-quality training data, you may reduce the scale of the noise over the course of training. TD3和DDPG一样，为了获得很好的探索效果，在训练阶段要给动作输出加一个噪声，并且随着训练的进行要逐渐减小噪声。
- At test time, to see how well the policy exploits what it has learned, we do not add noise to the actions. 测试阶段动作不加噪声。
- Our TD3 implementation uses a trick to improve exploration at the start of training. For a fixed number of steps at the beginning (set with the start_steps keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal TD3 exploration. 初期探索采用随机采样，之后才用智能体输出。
- 参考 [Stable Baselines3 TD3 教程](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html)，TD3相较于DDPG的三大改进点：clipped double Q-Learning、delayed policy update 和 target policy smoothing。
- TD3算法中涉及6个网络，分别是Actor Net、Critic1 Net和Critic2 Net及它们各自的目标网络。相较于DDPG，TD3增加了一对Critic Net。
    1. 三个目标网络的更新都是采用逐步软更新的方式。Actor Net每隔一定步数才更新一次（delay policy update），而两个在线Critic Net每步都更新。
    2. TD3算法中Critic Net的更新和DDPG算法中Critic Net的更新方式截然不同。在评估网络的更新中涉及两个trick（clipped double Q-learning 和 target policy smoothing）。Critic1/2 Net的更新基于TD error。TD3中贝尔曼方程右边的下一个动作是在Actor Target Net输出的基础上加上一个噪声（target policy smoothing），再分别用Critic1/2 Target Net得到两个Q值，取小的那一个（clipped double Q-learning）。
    3. Actor Net的更新方式与DDPG类似，都是优化Actor Net参数使得输出动作的Q值最大（输入经验中的state，输出action，用Critic1 Net计算Q值），只是在TD3中默认用Critic1 Net，不是每步都更新Actor Net，但Actor Target Net是每步都更新。

本日志详细梳理了TD3的核心思想与关键技术细节。
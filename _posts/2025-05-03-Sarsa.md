---
title: Sarsa，Sarsa与Q-learning的区别
description: 第一次知道Sarsa是在我研二的时候一次组会赵溧师兄讲强化学习时候听到的，当时并没有强化学习的知识储备，然后也没仔细深究，所以到今天都觉得很陌生，陌生感就会带来距离感，觉得这个东西棘手的很。但是读了这个知乎博客，我感觉Sarsa也就那么回事。
author: Chen Zhang
date: 2025-05-03 00:00:00 +0800
categories: [DRL]
tags: [Sarsa, DQN]
pin: true
math: true
mermaid: true
---

参考知乎文章[5.3.Sarsa](https://zhuanlan.zhihu.com/p/166412379)。

- 主要和Q-learning算法对比，他们的区别只有一处，就是在贝尔曼公式中计算下一状态的Q值的方式有所不同。
对于Q-learning计算下一状态的Q值时，首先是基于此时的策略来选出使得下一个状态的Q值最大的动作$a_{t+1}$，然后得到$Q(s_{t+1},a_{t+1})$；但是Sarsa不是，Sarsa的下一个动作和当前动作的获取方式一样也是基于贪心策略，或者当epsilon=0时，就是完全基于当前策略。
这是大多数博客这样区分他们两者的，而且代码里面也是这样，我是说Q-learning的代码和Sarsa的代码，因为Q-learning还没有引入经验回放。当我们引入经验回放，他们两个算法的区别就更加明显。
Q-learning的经验条目是$(s_{t},a_{t},r_{t},s_{t+1})$，Sarsa的经验条目是$(s_{t},a_{t},r_{t},s_{t+1},a_{s+1})$，这个形式也是Sarsa名字的由来-State Action Reward State Action名字是根据经验条目来取的。单单看这两个算法的经验条目的形式好像也没有发现他们两个的本质区别。其实关键点就在Sarsa经验数据中的最后一个数据$a_{t+1}$。我们应该用两个算法的Q值更新公式来区分:

$$
\begin{align}
DQN: a_{t + 1} = \mathop {\arg \max }\limits_{ {a_{t + 1}}} Q_{tar}\left( s_{t + 1}, a_{t + 1} \right), \\
Sarsa: {a_{t + 1}} = \mathop {\arg \max }\limits_{ {a_{t + 1}}} Q\left( { {s_{t + 1}},{a_{t + 1}}} \right){\space\space}if{\rm{\space\space}}\mu  \le \varepsilon, {\rm{\space\space}}else{\rm{\space\space}}rand, \\
\end{align}
$$

Q-learning的Q值更新公式右边计算下一时刻的Q值时是基于当前策略选出最佳动作并求出相应的Q值，然后来计算MSE最后更新策略；但是SarsaQ值的更新公式右边下一时刻Q值的计算是用之前策略下选出的动作来计算对应的Q值，来更新现在的策略，相当于用老思想来评判新思想。这就说明Sarsa的经验不能被后人学习，他只能用于当下，这就是On-policy。而O-learning不一样，它可以存放N年后重新拿来学习，这是off-policy。
- 简单区分一下Off-policy和On-policy：能用经验回放的就是off-policy，否则就是on-policy。

---
title: DDPG调优记录
description: 针对最小化N维向量平方和问题和单用户静态信道速率最大化问题，使用了DDPG和TD3来求解他们。做了一些实验并得出了一些经验，但是这些经验并不保证正确，还需要进一步验证。
author: Chen Zhang
date: 2025-05-20 00:00:00 +0800
categories: [Blogging]
tags: [Reinforcement Learning, DDPG, TD3, Wireless Communications]
pin: true
math: true
mermaid: true
---

[知乎专栏原文](https://zhuanlan.zhihu.com/p/84321382)

**个人实验记录与经验总结：**
- 从目前的实验结果来看，TD3并没有DDPG效果好，TD3在训练后期波动比DDPG大，而且收敛的值也比DDPG小。
- 前两天用DDPG求解这两个问题时，都出现了收敛到比较低的值的情况。针对这个问题，我将Actor和Critic的学习率从原来的5e-5,5e-5增大到1e-4,1e-3，调整之后效果显著增强。同时，也适当增大了学习轮数，缩小了状态空间。
- 考虑每轮训练选择不同的初始值，但是实验结果比较差，收敛似乎能很好的收敛，但是在测试时发现曲线很乱。
- 考虑在每轮训练结束对最后一步将done设置为true，但是实验结果表明不设置才能很好的收敛，设置之后似乎不太好收敛。
- 考虑DDPG探索性使用在Actor网络输出加上高斯噪声来实现，但是实验结果表明使用ε-greedy似乎更好。
- 考虑在环境step中对action进行限制，但是实验结果表明在网络输出时直接限制效果更好，但是在网络输出直接限制似乎收敛更慢。5月21日，似乎两者没有前面这样说的必然结果，我现在都是采用的网络输出为[-1,1]然后在环境中才限制动作范围，效果依然很好。
- 考虑ε-greedy中噪声的大小使用线性函数来实现随训练轮数增大噪声幅值减小的功能，实验结果表明线性函数比指数函数效果似乎更好。
- 利用DRL求解multi antenna/multi user的和速率最大化问题中，在之前的实验时，将奖励直接设置成和速率sumRate，发现最终很难收敛到最优值；现在打算把奖励设置成100^sumRate，也就是越靠近最优值给的奖励越大，进而鼓励agent往最优值的方向前进。一次实验结果显示，使用sumRate能收获更多的奖励。
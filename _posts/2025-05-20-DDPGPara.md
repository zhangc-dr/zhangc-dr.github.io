---
title: DDPG调优记录
description: 针对最小化N维向量平方和问题和单用户静态信道速率最大化问题，使用了DDPG和TD3来求解他们。做了一些实验并得出了一些经验，但是这些经验并不保证正确，还需要进一步验证。
author: Chen Zhang
date: 2025-05-20 00:00:00 +0800
categories: [DRL]
tags: [DDPG, TD3]
pin: false
math: true
mermaid: true
---

[知乎专栏原文](https://zhuanlan.zhihu.com/p/84321382)

**个人实验记录与经验总结：**
- 从目前的实验结果来看，TD3 并没有 DDPG 效果好，TD3 在训练后期波动比 DDPG 大，而且收敛的值也比 DDPG 小。
- 前两天用 DDPG 求解这两个问题时，都出现了收敛到比较低的值的情况。针对这个问题，我将 Actor 和 Critic 的学习率从原来的 5e-5, 5e-5 增大到 1e-4, 1e-3，调整之后效果显著增强。同时，也适当增大了学习轮数，缩小了状态空间。
- 考虑每轮训练选择不同的初始值，但是实验结果比较差，收敛似乎能很好的收敛，但是在测试时发现曲线很乱。
- 考虑在每轮训练结束对最后一步将 done 设置为 true，但是实验结果表明不设置才能很好的收敛，设置之后似乎不太好收敛。
- 考虑 DDPG 探索性使用在 Actor 网络输出加上高斯噪声来实现，但是实验结果表明使用 ε-greedy 似乎更好。
- 考虑在环境 step 中对 action 进行限制，但是实验结果表明在网络输出时直接限制效果更好，但是在网络输出直接限制似乎收敛更慢。5月21日，似乎两者没有前面这样说的必然结果，我现在都是采用的网络输出为 [-1,1] 然后在环境中才限制动作范围，效果依然很好。
- 考虑 ε-greedy 中噪声的大小使用线性函数来实现随训练轮数增大噪声幅值减小的功能，实验结果表明线性函数比指数函数效果似乎更好。
- 利用 DRL 求解 multi antenna/multi user 的和速率最大化问题中：
    - 在之前的实验时，将奖励直接设置成和速率 sumRate，发现最终很难收敛到最优值。
    - 现在打算把奖励设置成 $100^{sumRate}$，也就是越靠近最优值给的奖励越大，进而鼓励 agent 往最优值的方向前进。
    - 一次实验结果显示，使用 sumRate 能收获更多的奖励。

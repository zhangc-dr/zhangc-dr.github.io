---
title: PPO详细的公式推导与李宏毅PPO课程学习笔记
description: 结合B站PPO推导视频与台湾李宏毅教授PPO课程的学习笔记与个人思考，记录PPO相关公式推导、优势函数、目标网络等知识点，及强化学习Q-learning/DQN/DDQN等算法的理解与总结。
author: Chen Zhang
date: 2025-04-22 00:00:00 +0800
categories: [Blogging]
tags: [Reinforcement Learning]
pin: true
math: true
mermaid: true
---

**个人学习与思考摘要：**

{% include embed/youtube.html id='OAKAZhFmYoI' %}
> 待解决的问题：为什么采用优势函数？为什么希望R这一项有正有负？为什么改写R？
{: .prompt-warning }

#### 1. 如何理解价值网络需要目标网络？
 - 4月29日，学习完DDPG、PPO、TD3之后，再来系统性学习强化学习，今天决定接着看李宏毅老师的强化学习课程。在这个专栏的Lecture 3 Q-learning的中，他讲到了为什么要给Q网络引入Target网络？对于这个问题，我们首先需要知道Q值怎么得到的。其实是根据公式$Q(s_{t},a_{t})=rt+Q(s_{t+1},a_{t+1})$来不断更新的，用左边不断逼近右边。但是要知道，在逼近过程中$Q(s_{t},a_{t})$会发生改变，不止是$Q(s_{t},a_{t})$变化，整个Q网络的参数都会变，那么$Q(s_{t+1},a_{t+1})$也就变了，现在的问题变得复杂了。
 - 举个例子，假设你现在要追你家猫，要是它带那不动你总会逮到它；那万一这猫乱跑呢，甚至它速度时快时慢，这你不得也跟着乱跑啊，它要跑到树上你也得跟上去，它要是跑到房顶你也得爬上去，这多累啊，这什么时候能追到啊，而且猫看到你一直追它，它就一直跑，没辙。所以你现在决定每隔一段时间把猫某个时刻的位置记下来，把这个位置当作你假想的静态目标，你就拼命往这个目标跑而不是一直追着猫跑，慢慢的，猫发现你没有一直追它，它就放松下警惕，渐渐的就慢下来了，这样你就追上它了。
 - 回到Q网络，DQN、DDPG这些算法确实是这么干的，它们的Q网络都会配备一个Target网络，这样才能保证Q网络不会乱优化，毕竟固定靶子要比移动靶好射一些。
#### 2. 强化学习为什么会陷入局部最优？
 - 4月29日，同样是Lecture 3 Q-learning的视频中，38min左右李老师提到数据问题是Q-learning面临的很严重的问题。回顾Q-learning的思路，核心思路就是训练一个Q函数，它能指导你的行动，跟着Q函数走，你永远都可以获得它认为的最多奖励的路径。你要做的就是训练一个足够完美的Q函数，不断提高它的认知水平，让它尽可能逼近理想Q函数，这样你就一定能获得一条趋近完美的路径，拿到最多的奖励。但是不幸的是，这么一个函数很难训练到极致，这个视频中李老师说的问题就是数据的问题，要想训练一个最优的Q函数不得不获得几乎全部可能的$(s_{t},a_{t})$ pair，但是这可想而知不太可能，对于简单的离散的场景那还可以实现，但是稍微复杂一些几乎就不可能了。得不到这些数据就很难训练出一个完美Q函数，这是关于数据的其中一个问题。还有一个问题，假设现在训练到中间某一个阶段，此时有一个还待完善的Q函数，训练Q网络过程数据的来源分为探索和利用。在利用阶段，智能体在Move的时候会根据Q函数来指导它，这时候我可以把这个Q函数理解成认知不高的人来指导你，明明往别的方向走可以获得更多的奖励，但是没办法它认知有限。为什么会出现这种情况呢？
 - 可以举一个例子来说明，你是一个大学生小刘，每天都要吃饭，但你每次不知道吃什么好，于是你想到了做一张评分表，每次吃饭就按这个表来，哪个窗口哪道菜好吃，你就给它打高分，不好吃，你就打低分甚至负分。前几次，因为你刚开始执行嘛，表是个空的，你就需要多去试，每次都吃不一样的菜，然后把分数填在表上，这样吃了一段时间，你发现好像试的差不多了，你觉得你根据这个表就能每次都能吃上最好吃的饭菜了。于是你就开始每次吃饭都去查这个表格，每次都去分最高的那家。这样搞了几次，有一天你一个同学也面临吃什么的问题，这时候你信心满满的拿出你的评分表，指着一道分数最高的菜说这个菜巨好吃，就吃这个，信我妥妥的。可是没想到，你这同学看了一眼你推荐的这道菜，笑着说道啥呀，这还没我上回吃的藕带鸡杂好吃。完了，天塌了，你看了一下你的表格发现你甚至都还没吃过这道菜，你认为无敌的一张表格，到你同学这啥都不是。怎么办呢？于是你就决定要更新你的表格，你要去试试别的没尝过的菜。但是，要知道尝试总会面临收益和风险的嘛，于是，你选择不能太激进，不能一次性全试一遍，万一你运气不好试的这几道菜都难吃怎么办。于是，你就想到了我要开开心心的把这个表完善好，周一到周五要上课你就全按表上来，这几天天天吃好吃的，到了周六周日，就去尝试没吃过的。这样一来，最后你得到了全校唯一一份饭菜评分表，有谁不知道吃啥的时候，你就可以把这表拍他脸上，说：按哥这表来，你错不了。没错，你这时候的表格已经是最完美的表格了。
 - 言归正传，回到Q-learning问题来，为什么说明明有更好的路径可以走，偏偏固执的走另一条路呢。这就是因为认知缺陷，自己过于迷信Q函数了，掉入了认知陷阱。
#### 3. 新的soft policy-Boltzmann Exploration
- 4月29日，同样是Lecture 3 Q-learning，视频40min左右提到了避免掉入认知缺陷的方法，一种就是贪心策略，就类似觉悟后的小刘；另一种是Boltzmann Exploration，不同于贪心策略，贪心策略是有一定概率去探索新东西，有一定概率根据Q函数来做肯定的决定。
- 但是Boltzmann Exploration思路完全不同，小刘依然每次都要带着评分表，而且都要做好记录，但不一样的是，他不再是根据最高分的菜来挑选了，而是拿着一个特制的骰子，这个骰子和常见的可能不一样，它不是一个每个面都均匀的骰子了，它会根据打分表的大小设计好，保证分数高的投中的概率大些，分数低的投中的概率小些。这样就能保证既不会承担很大的试错的风险，也不必担心你会陷入认知陷阱中，因为尽管那些没吃过的被摇到的概率很小（这里完善一下小刘这个例子的故事设定，假设初始评分表不是全空的，而是每道菜的分数都很低，可能是十的负5次方），但是总会有可能出现。
- 忘记了一个重要的东西，Boltzmann Exploration到底是什么还没说呢。$P(a|s)=e^{Q(s,a)}/sum(Q)$，在训练过程中就按照这个概率去Move，这就是Boltzmann Exploration。
- 我一直以为Q-learning、DQN、DDPG之类的确定策略最多能存在的随机只能是贪心算法这种类型的随机，没想到可以把随机选动作和根据Q函数确定性得到动作结合起来变成一个把Q函数确定性嵌套在随机里面，变成一个<确定的随机>。
#### 4. 为什么一定要探索+利用，为什么不是只探索？
- 4月29日，关于Q-learning我还想到了一个问题，为什么一定要探索+利用，为什么不是只探索，完全随机的去探索每一个(state, action) pair，这样不是能更好的解决数据不完备的问题吗？
- 小刘不就很快就得到了一张完美的评分表了吗？原因我思考了一下，小刘为什么不一次性试完所有食堂之后在开始使用他的评分表呢？是因为他害怕万一连续几天都吃难吃的要死的饭呢，本来学习就累的够呛，结果吃的饭还难以下咽，这谁受得了。所以他选择我多吃好吃的，偶尔去试试别的没试过的，万一遇到更好吃的呢，要是遇到不好吃的直接避雷。那么在Q-learning中，首先获取完备的数据就已经很难了，然后如果利用现有的Q函数，就相当于利用以前的经验嘛，如果完全不用以前的经验，那太遭罪了，不白费了嘛，用以前的经验再搭配试错的勇气，就能更快的获得最完美的Q函数。
- 4月30日，回头看这段话，发现不太对，换成就能慢慢的去逼近最完美的Q函数，因为连续空间中的样本几乎不可能完全获取到，总是只能获取到局部样本。只获取局部样本会导致什么问题呢前面也说的很清楚了，那就相当于小刘并没有尝试完所有的菜就说自己的评分表天衣无缝，他陷入到自己认知下的最优假象。
#### 5. 无目标网络DQN、DQN和DDQN的区别
- 4月30日，看完李老师对DQN和DDQN的原理的介绍，并理解了YouTube博主Phil的代码实现中算法的细节之后，试图总结一下无目标网络DQN、DQN和DDQN的区别。
- 还是以追猫为例子，之前说过无目标网络DQN相当于眼睛盯着猫，一直跟着它，这样做很容易导致猫一直被你刺激然后不停的乱跑，这样很难追上；而加了目标网络之后，就会每隔一段时间给自己设置假想目标，这个假想目标可不是瞎弄的，每个时刻段末你都记下猫的位置，然后以这个位置作为自己下一个时间段的假想目标，这就是假想目标得到的方式，然后眼睛盯着这个假想目标腿就去跑向这个目标，猫知道你没一直跟着它，它就逐渐放慢脚步，你慢慢的就能追上它了；而新来的DDQN呢，它还是会隔段时间给自己定一个假想目标，但是和DQN不同的是你眼睛还是盯着这个目标但是你的腿却不太愿意听眼睛的，可能觉得眼睛盯着的这个目标不够好，它们更希望一直追着猫跑，但是又不得不听眼睛的，因为眼睛会给脑子打报告，让脑子来收拾腿，让腿不得不一边追着猫，一边得听眼睛的往假想目标跑。
- 最后回到这几个网络中，从网络结构上总结一下他们，没有目标网络的DQN只需要一个估计网络；DQN和DDQN都引入了目标网络，它们都有两个网络，分别是估计网和目标网络，要记住DDQN并没有相较于DQN新引入别的网络，不要被它的Double唬住了。最后，从网络更新方式上总结，这里主要说估计网络的更新方式，因为目标的更新很简单，就只是隔段时间把估计网络的参数复制过来。对于目标网络，他们都是根据贝尔曼方程来的，具体来说就是让贝尔曼方程左右两边差值缩小，就相当于人追猫，最后希望能追到。
- 不对！我好像举的这个追猫的例子不太对，无目标网络的DQN和DQN的区别可以用那个例子来举例，但是DQN和DDQN的区别不能这样举例，腿相当于网络更新的方向，眼睛和腿一定始终保持协同一致，因为腿只能根据眼睛看到的去行动，眼睛看到的相当于MSE，而MSE决定了梯度方向，也就是腿的方向，所以前面说的腿不听眼睛的是不成立的。DQN和DDQN真正不同的是，DQN中目标的确定和DDQN的目标不一样。把这个例子改成，DQN的假想目标只和上回记下来猫的位置有关，上回猫印象中猫的位置就是我要跑过去的目标位置；而DDQN的假想目标既和上回记下的猫的位置有关也和现在看到猫的位置有关，怎么理解呢。就相当于你明明在朝着假想位置跑，但是你眼睛却也盯着猫现在的位置，完了，你记下来的猫上个时刻的位置在哪来着，被刚刚猫的身影搅乱了，就相当于你现在的目标受到了此时猫的位置的影响了。
- 那么接着说网络更新方式的区别，无目标网络的DQN的更新方式很简单就是方程左边用估计网络得到当前状态做当前动作的Q值，方程右边，首先根据估计网络确定下一个状态对应的最优动作，然后还是用估计网络得到下一个状态以及刚刚得到的下一个最优动作的Q值乘上折扣因子，再加上当前状态做当前动作得到的单步奖励，这就是方程右边。
- 然后算两者的MSE，然后求梯度，然后用这个梯度去更新估计网络。对于DQN，和无估计网络方程左边是一样的都是用估计网络得到当前状态和当前动作对应的Q值，不同的是方程右边中下一个状态采取的最优动作，以及下一个状态和这个下一个最优动作对应的Q值的计算。在DQN中，下一个状态采取的最优动作不再是直接由估计网络得到，而是由目标网络得到，然后下一个状态和这个下一个最优动作对应的Q值也是根据目标网络得到。
- 最后，DDQN的更新方式在DQN的基础上又有所不同，方程左边这三个算法的计算方式都是一样的，对于DDQN中方程的右边和DQN的区别只是下一个状态的最优动作不是由目标网络得到的，而是由估计网络得到的，而下一个状态和这个下一个最优动作对应的Q值则还是根据目标网络得到的，公式如下：
$$
\begin{align}
DQN: a_{t + 1} = \mathop {\arg \max }\limits_{{a_{t + 1}}} Q_{tar}\left( s_{t + 1}, a_{t + 1} \right), MSE = \left|  r_t +\gamma Q_{tar}(s_{t+1}, a_{t+1}) - Q_{eval}(s_t, a_t) \right|^2, \\
DDQN: a_{t + 1} = \mathop {\arg \max }\limits_{{a_{t + 1}}} Q_{eval}(s_{t + 1}, a_{t + 1}), MSE = \left|  r_t +\gamma Q_{tar}(s_{t+1}, a_{t+1}) - Q_{eval}(s_t, a_t) \right|^2.
\end{align}
$$
#### 6. 如何用Q-learning求解连续动作问题?
- 5月3日，Lecture 5中李老师讲到如何用Q-learning求解连续动作问题。用Q-learning求解连续动作面临的问题主要是在选择动作时无法按离散动作采取的argmaxQ的方式来选择动作，对于离散动作，在利用策略选择动作时是将状态代入Q函数，然后得到这个状态下一些列动作的Q值，这一系列Q值是有限个的，然后挑选一个最大Q值对应的action；现在对于连续动作，在利用策略选择动作时，代入state到Q函数得到的就不再是有限个Q值，因为动作是连续的，所以Q(st，at)|at对应的就是一个连续的函数，要从中挑选最优action就需要这个函数Q(st,at)|at对action求偏导得到最优action，这会带来很大的计算压力（paper中提到：每次更新时都需要对复杂的非线性函数进行最大化）。
- 方法1，离散化动作空间，然后再用传统DQN来解这个问题。这个很好理解；方法2，参考一篇论文Continuous Deep Q-Learning with Model-based Acceleration 对于这个方法，需要重新设计Q网络结构，而且需要改变Q函数的求解方式为Q(st,at)=A(st,at)+V(st)。但对于Q函数的计算仅仅是这样做了个变换是不够的，因为A(st,at)中依然存在连续的at不好处理，需要通过求偏导来得到最优at，这样会增大计算负担是不可取的（李老师视频中还说到这样做还会导致陷入局部最优，我目前只能理解增大计算负担）。paper中提出的方法就是找到一个函数用来拟合A(st,at)，使得不需要A(st,at)对at求偏导就能获取最优at。paper找到的这个函数是一个二次函数，这个二次函数由两个参数确定，一个是μ另一个是Σ，他们的维度取决于action的维度，公式如下：
$$
\begin{align}
{\rm{Q}}\left( {{s_t},{a_t}} \right) = A\left( {{s_t},{a_t}} \right) + V \left( {{s_t}} \right),A\left( {{s_t},{a_t}} \right) =  - 0.5{\left( {{a_t} - \mu \left( {{s_t}} \right)} \right)^{\rm{T}}}\Sigma \left( {{s_t}} \right)\left( {{a_t} - \mu \left( {{s_t}} \right)} \right), \\
MSE = {\left| {{r_t} + \gamma V\left( {{s_{t + 1}}} \right) - Q\left( {{s_t},{a_t}} \right)} \right|^2}
\end{align}
$$

- 但我觉得这和随机策略处理连续动作的方式如出一辙，都是针对一个不便拟合的连续函数去用只有少量参数的常规函数拟合。这里用的是二次函数，随即策略用的是高斯分布的概率密度函数，都只需要两个参数。但是很显然这样去拟合会损失掉被拟合函数的大量信息。这里解释一下为什么我说二次函数也只有两个参数，正常二次函数是y=ax^2+bx+c有三个参数，但这里是令c=0，所以只有两个参数。
#### 7. 动作空间从离散扩展成连续的时候会遇到拟合导致的信息损失问题
- 5月3日，突然发现基于策略的方法和基于价值的方法，在动作空间从离散扩展成连续的时候都会遇到拟合导致的信息损失问题。当动作空间离散时，基于策略的方法的思路是训练出一个策略，这个策略是关于$P(st|at)$的，利用这个策略做出动作使得总收益的期望最大化，最终就是为了得到$P(st,a1,t),P(st,a2,t),...,P(st,aN,t)$这些概率，N是所以可能的动作数量。网络的输出就是某个状态下每个动作被采用的概率，用softmax作为输出层，输出层输出维度等于动作数，每个动作一个概率；基于价值的方法的思路是训练一个策略，这个策略是关于$Q(st,at)$的，利用这个策略使得总收益最大化，用TD方法来更新Q网络，最终就是为了得到Q函数。网络的输出就是在某个状态下每个动作的Q值，用线性层作为输出层，输出层输出维度等于动作数，然后用argmax函数来取出输出层最大输出的index就是action的index，然后就得到了最优动作。注意到这两种方法的输出层输出维度是等于动作数的。那么如果动作数逐渐增加，输出层输出维度就也要随之增加，当动作数很多时，可想而知，网络输出层的节点数需要很多才足以对应每个动作的概率或者Q值。
- 按照这个思路，当动作空间为连续时，输出层输出维度可想而知应当为无穷个，或者说这时候需要输出的是一个连续函数不再是一个离散函数，但是如何去输出一个函数呢？网络的输出，但是基于策略和基于价值的方法在处理这个问题上是分别用高斯函数拟合连续的概率密度函数、用二次函数拟合优势函数。单单说波峰的数量，用这两种单峰的函数去拟合一个可能有多个峰的连续函数就不合适。
- 但是用这两种函数拟合有一个好处，那就是在根据概率密度函数或者Q函数求最大动作时，如果不用这两种函数去拟合就需要对概率密度函数或Q函数求偏导得到最优动作，但这样做会带来计算上的压力。而采用这两种函数去拟合时，拟合概率密度函数时，高斯函数的期望就是概率最大的动作；拟合Q函数时，二次函数中当at等于μ(st)时，Q值最大，此时的动作就是最优动作。综上，采用高斯概率密度函数或二次函数分别拟合状态st的概率密度函数或Q函数时，缺点是信息损失；优点是易于计算最优动作。
#### 8. 离散动作空间的动作数和连续动作空间中的动作维度混淆
- 5月3日，我在写代码时一直每太理清楚离散动作空间的动作数n_actionSpace和连续动作空间中的动作维度action_dim。
- 动作维度很好理解就是系统的自由度，对于一个一维空间的移动，此时你只能在这一个方向上向前走或者向后退，就只有一个自由度，那么动作维度就只有1；对于一个二维空间的移动，此时你能在x轴和y轴上移动，自由度为2，那么动作维度就是2。
- 动作维度部分连续还是离散，其实离散也有动作维度，它和连续动作的区别是，离散动作会在连续动作的每个维度进行采样构成离散动作空间，每个维度采样数相乘就是离散动作的动作数，这个动作数其实是把每个维度采样的动作进行排列组合之后的所有动作组合的组合数。也就是说离散动作完全可以表示成多个维度，这样的话网络结构就会有所不同。
- 不考虑动作维度的离散动作的网络输出层是用softmax，输出节点之和等于1，就相当于把所有组合的概率求和，他是恒等于1的。而考虑动作维度的离散动作的网络输出层同样还是用softmax，不同的是输出节点被分成action_dim个组，组内概率之和等于1.然后在计算回报的期望时要把所有维度的概率乘起来得到组合概率。
- 其实区别就是到底是选择组合动作还是选择组合概率。现有的代码是组合动作，而组合概率的想法可以用代码实现一下，对比两种的区别。
#### 9. 为什么DDPG中给Actor Net也增加了一个目标网络？
- 5月6日，李老师视频 Lecture 6 Actor-critic中讲到了为什么DDPG中给Actor Net也增加了一个目标网络，Critic Net加一个目标网络很好理解，这在DQN中就给Q网络增加了一个目标网络，就是希望靶子不要一直动。
- 视频中讲到增加Actor Target Net的目的其实也是为了稳定Critic Net，对！不是稳定Actor Net，而是Critic Net。回想Critic Net的更新方式，采用的是TD error，减号左边是根据在线的Critic Net来计算当前状态采取当前动作的Q值，右边是reward加上下一状态以及下一状态采取的动作对应的Q值，下一状态对应的下一个动作怎么计算的？没错是根据Actor Net来得到的，说的通俗点就是根据当前Actor Net的参数对应的准则来得到的。
- 由于Actor Net每一步都在被更新，但是如果现在不加入Actor Target Net那么这个下一个动作的选择准则一直在变动，导致计算下一个状态以及下一个动作对应的Q值一直在乱变，所以要固定住Actor Net，方式就是新增一个Actor Target Net，每隔一段时间更新Actor Target Net这个网络。

